SOC4Kafka (Splunk OpenTelemetry Collector for Kafka) has been installed.

1. Configure your Kafka receivers and Splunk exporters:

   Create or update a values.yaml file with your configuration:
   
   # Example values.yaml:
   kafkaReceivers:
     - name: main
       brokers:
         - "broker1:9092"
         - "broker2:9092"
       logs:
         topics:
           - "your-topic"
         encoding: text
       group_id: "soc4kafka-main"
   
   splunkExporters:
     - name: primary
       endpoint: "https://splunk.example.com:8088/services/collector"
       secret: "my-splunk-hec-secret"  # Or use token: "YOUR_TOKEN"
       source: "soc4kafka"
       sourcetype: "otel:logs"
       index: "your-index"
   
   pipelines:
     - name: main-logs
       type: logs
       receivers:
         - main
       exporters:
         - primary
       processors:
         - batch
         - resourcedetection
   
   Then upgrade:
   helm upgrade {{ .Release.Name }} . -f values.yaml

2. If using secrets, create them first:
   
   kubectl create secret generic my-splunk-hec-secret \
     --from-literal=splunk-hec-token=YOUR_HEC_TOKEN
   
   # For Kafka authentication:
   kubectl create secret generic kafka-auth-secret \
     --from-literal=password=YOUR_KAFKA_PASSWORD

3. Check collector pods:

   kubectl get pods -l app.kubernetes.io/name={{ include "soc4kafka.name" . }}
   kubectl logs -l app.kubernetes.io/name={{ include "soc4kafka.name" . }} -f

4. Check collector health (if service is enabled):

   kubectl port-forward svc/{{ include "soc4kafka.fullname" . }} 13133:13133
   curl http://localhost:13133/
   
   # Or directly via pod:
   kubectl port-forward -l app.kubernetes.io/name={{ include "soc4kafka.name" . }} 13133:13133

5. On MicroK8s: ensure the cluster has access to Kafka (in-cluster or network reachable)
   and that the Splunk HEC endpoint is reachable from the cluster.

Documentation: https://github.com/splunk/splunk-opentelemetry-collector-for-kafka
